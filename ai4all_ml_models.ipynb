{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwZu30RXYikX4pR3IzRwT0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikhlaspalakkattu/ai4all6b/blob/main/ai4all_ml_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5rKIs12AS-L",
        "outputId": "9664f1d7-6278-42c0-cae7-d4d2102981ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged CSV saved as 'Combined_Historical_Data.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# File names and their corresponding ticker symbols\n",
        "files = [\n",
        "    (\"AMZN_Historical_Data.csv\", \"AMZN\"),\n",
        "    (\"FB_Historical_Data.csv\", \"FB\"),\n",
        "    (\"GOOG_Historical_Data.csv\", \"GOOG\"),\n",
        "    (\"GOOGL_Historical_Data.csv\", \"GOOGL\"),\n",
        "    (\"NFLX_Historical_Data.csv\", \"NFLX\"),\n",
        "    (\"AAPL_Historical_Data.csv\", \"AAPL\")\n",
        "]\n",
        "\n",
        "# Date range for filtering\n",
        "start_date = pd.to_datetime(\"2012-01-01\")\n",
        "end_date = pd.to_datetime(\"2023-01-01\")  # inclusive up to this date\n",
        "\n",
        "merged_df = pd.DataFrame()\n",
        "\n",
        "for filename, ticker in files:\n",
        "    # Read CSV, parse 'Date' as a datetime\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    # Make sure the Date column is correctly formatted\n",
        "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    # Drop rows where Date couldn't be parsed\n",
        "    df = df.dropna(subset=['Date'])\n",
        "\n",
        "    # Filter rows by date range (inclusive, if you want strictly before 2023-01-01 change <= to <)\n",
        "    mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n",
        "    df = df.loc[mask].copy()\n",
        "\n",
        "    # Add Ticker column\n",
        "    df['Ticker'] = ticker\n",
        "\n",
        "    # Optional: Clean numeric columns (remove commas, convert to numbers)\n",
        "    for col in ['Price', 'Open', 'High', 'Low', 'Vol.', 'Change %']:\n",
        "        if col in df.columns:\n",
        "            if col == 'Vol.':\n",
        "                # Typically 'Vol.' may have K/M/B suffixes. You may want to extend parsing if that's the case.\n",
        "                df[col] = df[col].astype(str).str.replace(',', '')\n",
        "            else:\n",
        "                df[col] = df[col].astype(str).str.replace(',', '')\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
        "\n",
        "# Save to csv\n",
        "merged_df.to_csv(\"Combined_Historical_Data.csv\", index=False)\n",
        "\n",
        "print(\"Merged CSV saved as 'Combined_Historical_Data.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load sentiment and stock article datasets\n",
        "sentiment_df = pd.read_csv(\"sentiment_labeled_dataset.csv\")\n",
        "articles_df = pd.read_csv(\"stock_data_articles.csv\")\n",
        "price_df = pd.read_csv(\"Combined_Historical_Data.csv\")  # from previous merging step\n",
        "\n",
        "# Preprocess datasets\n",
        "sentiment_df[\"title\"] = sentiment_df[\"title\"].str.strip().str.lower()\n",
        "articles_df[\"Title\"] = articles_df[\"Title\"].str.strip().str.lower()\n",
        "\n",
        "# Merge news with sentiment using the 'title' field\n",
        "merged_articles = pd.merge(\n",
        "    articles_df, sentiment_df, left_on='Title', right_on='title', how='inner'\n",
        ")\n",
        "\n",
        "# Convert publish date to datetime and join to stock price (based on symbol and date)\n",
        "merged_articles['Publishdate'] = pd.to_datetime(merged_articles['Publishdate'])\n",
        "price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
        "final_df = pd.merge(\n",
        "    merged_articles,\n",
        "    price_df,\n",
        "    left_on=['symbol', 'Publishdate'],\n",
        "    right_on=['Ticker', 'Date'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Create target: 1 if price increases tomorrow, 0 otherwise\n",
        "final_df = final_df.sort_values(['symbol', 'Publishdate'])\n",
        "final_df['Next_Price'] = final_df.groupby('symbol')['Price'].shift(-1)\n",
        "final_df['Price_Change'] = final_df['Next_Price'] - final_df['Price']\n",
        "final_df['Target'] = (final_df['Price_Change'] > 0).astype(int)\n",
        "\n",
        "# Feature Engineering: use emotion scores, label, etc.\n",
        "feature_cols = ['sad', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "X = final_df[feature_cols]\n",
        "y = final_df['Target']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train a simple logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5xFZo9rDxLl",
        "outputId": "007e3382-66ff-4fe1-b897-3b369bffe7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.67      1.00      0.80         4\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.33      0.50      0.40         6\n",
            "weighted avg       0.44      0.67      0.53         6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Read the merged data (as explained previously)\n",
        "# sentiment_df: sentiment dataset\n",
        "# articles_df: articles dataset\n",
        "# price_df: historical prices\n",
        "\n",
        "# 1. Load the data\n",
        "sentiment_df = pd.read_csv(\"sentiment_labeled_dataset.csv\")\n",
        "articles_df = pd.read_csv(\"stock_data_articles.csv\")\n",
        "price_df = pd.read_csv(\"Combined_Historical_Data.csv\")\n",
        "\n",
        "# 2. Standardize titles for merging\n",
        "sentiment_df[\"title\"] = sentiment_df[\"title\"].str.strip().str.lower()\n",
        "articles_df[\"Title\"] = articles_df[\"Title\"].str.strip().str.lower()\n",
        "\n",
        "# 3. Merge news articles with sentiment data\n",
        "merged_articles = pd.merge(\n",
        "    articles_df, sentiment_df, left_on='Title', right_on='title', how='inner'\n",
        ")\n",
        "\n",
        "# 4. Merge with stock prices based on symbol and date\n",
        "merged_articles['Publishdate'] = pd.to_datetime(merged_articles['Publishdate'])\n",
        "price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
        "final_df = pd.merge(\n",
        "    merged_articles,\n",
        "    price_df,\n",
        "    left_on=['symbol', 'Publishdate'],\n",
        "    right_on=['Ticker', 'Date'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# 5. Feature engineering\n",
        "final_df = final_df.sort_values(['symbol', 'Publishdate'])\n",
        "final_df['Next_Price'] = final_df.groupby('symbol')['Price'].shift(-1)\n",
        "final_df['Price_Change'] = final_df['Next_Price'] - final_df['Price']\n",
        "final_df['Target'] = (final_df['Price_Change'] > 0).astype(int)\n",
        "\n",
        "# Drop any rows with missing data\n",
        "final_df = final_df.dropna(subset=['Target', 'sad', 'joy', 'love', 'anger', 'fear', 'surprise'])\n",
        "\n",
        "# 6. Feature selection\n",
        "features = ['sad', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "X = final_df[features]\n",
        "y = final_df['Target']\n",
        "\n",
        "# 7. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "\n",
        "# 8. Model training with XGBoost\n",
        "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 9. Predictions and evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkpUhxWsEYuq",
        "outputId": "9d44c461-7656-452d-b7f2-b01dabe70295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.67      1.00      0.80         4\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.33      0.50      0.40         6\n",
            "weighted avg       0.44      0.67      0.53         6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [02:16:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pmdarima if not installed\n",
        "\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    import pmdarima as pm\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pmdarima\"])\n",
        "    import pmdarima as pm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- Step 1: Load your datasets ---\n",
        "price_df = pd.read_csv(\"Combined_Historical_Data.csv\")\n",
        "sentiment_df = pd.read_csv(\"sentiment_labeled_dataset.csv\")\n",
        "articles_df = pd.read_csv(\"stock_data_articles.csv\")\n",
        "\n",
        "# --- Step 2: Preprocessing titles and merging sentiment with articles ---\n",
        "sentiment_df[\"title\"] = sentiment_df[\"title\"].str.strip().str.lower()\n",
        "articles_df[\"Title\"] = articles_df[\"Title\"].str.strip().str.lower()\n",
        "\n",
        "merged_articles = pd.merge(\n",
        "    articles_df,\n",
        "    sentiment_df,\n",
        "    left_on='Title',\n",
        "    right_on='title',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "merged_articles['Publishdate'] = pd.to_datetime(merged_articles['Publishdate'])\n",
        "price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
        "\n",
        "# --- Step 3: Aggregate daily sentiment scores ---\n",
        "sentiment_cols = ['sad', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "daily_sentiment = merged_articles.groupby('Publishdate')[sentiment_cols].mean().reset_index()\n",
        "\n",
        "# --- Step 4: Select a stock ticker and merge price with sentiment ---\n",
        "ticker = 'AAPL'  # Change this as needed\n",
        "stock = price_df[price_df['Ticker'] == ticker][['Date', 'Price']].sort_values('Date')\n",
        "\n",
        "# Merge prices with daily sentiment\n",
        "data = pd.merge(\n",
        "    stock,\n",
        "    daily_sentiment,\n",
        "    left_on='Date',\n",
        "    right_on='Publishdate',\n",
        "    how='left'\n",
        ")\n",
        "data = data.set_index('Date')\n",
        "\n",
        "# --- Step 5: Reindex to continuous daily dates and fill missing ---\n",
        "full_idx = pd.date_range(start=data.index.min(), end=data.index.max(), freq='D')\n",
        "data = data.reindex(full_idx)\n",
        "data.index.name = 'Date'\n",
        "\n",
        "data[sentiment_cols] = data[sentiment_cols].fillna(0)\n",
        "data['Price'] = data['Price'].ffill()\n",
        "\n",
        "# --- Step 6: Compute log returns for stationarity ---\n",
        "data['LogPrice'] = np.log(data['Price'])\n",
        "data['LogReturn'] = data['LogPrice'].diff()\n",
        "data = data.dropna(subset=['LogReturn'])\n",
        "\n",
        "# Align exogenous variables with returns\n",
        "exog = data[sentiment_cols]\n",
        "\n",
        "# --- Step 7: Split into train/test ---\n",
        "train_size = int(len(data)*0.9)\n",
        "train_endog = data['LogReturn'][:train_size]\n",
        "test_endog = data['LogReturn'][train_size:]\n",
        "train_exog = exog[:train_size]\n",
        "test_exog = exog[train_size:]\n",
        "\n",
        "# --- Step 8: Use auto_arima to find best SARIMAX order ---\n",
        "print(\"Finding optimal SARIMAX order by auto_arima, this may take a while...\")\n",
        "auto_model = pm.auto_arima(train_endog,\n",
        "                           exogenous=train_exog,\n",
        "                           seasonal=False,\n",
        "                           stepwise=True,\n",
        "                           suppress_warnings=True,\n",
        "                           error_action='ignore',\n",
        "                           trace=True,\n",
        "                           max_p=5, max_q=5, max_d=2)\n",
        "\n",
        "print(f\"Best SARIMAX order found: {auto_model.order}\")\n",
        "\n",
        "# --- Step 9: Fit SARIMAX with best order ---\n",
        "model = SARIMAX(train_endog,\n",
        "                exog=train_exog,\n",
        "                order=auto_model.order,\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False)\n",
        "\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# --- Step 10: Forecast log returns on test set ---\n",
        "start = train_size\n",
        "end = len(data) - 1\n",
        "pred_returns = model_fit.predict(start=start, end=end, exog=test_exog)\n",
        "\n",
        "# --- Step 11: Invert log returns to price forecast ---\n",
        "last_train_price = data['Price'].iloc[train_size - 1]\n",
        "price_forecast = [last_train_price * np.exp(pred_returns.iloc[0])]\n",
        "for r in pred_returns.iloc[1:]:\n",
        "    price_forecast.append(price_forecast[-1] * np.exp(r))\n",
        "\n",
        "forecast_index = data.index[train_size:]\n",
        "\n",
        "# --- Step 12: Evaluation ---\n",
        "rmse = np.sqrt(mean_squared_error(data['Price'][train_size:], price_forecast))\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "# --- Step 13: Plot actual vs forecasted prices ---\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(data.index, data['Price'], label='Actual Price')\n",
        "plt.plot(forecast_index, price_forecast, label='SARIMAX Forecast', color='green')\n",
        "plt.axvline(data.index[train_size], color='red', linestyle='--', label='Train/Test Split')\n",
        "plt.title(f\"SARIMAX Forecast of {ticker} Stock Price Using Sentiment Exogenous Features\")\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "gRMCkacpEmrY",
        "outputId": "94bc3bc1-c401-429b-e86b-f71be59b81a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-393048228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpmdarima\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pmdarima\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Stuff we want at top-level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marima\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_arima\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoARIMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStepwiseContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocorr_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_acf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_pacf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtsdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/arima/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapprox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marima\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/arima/approx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_endog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/utils/array.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_array\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mC_intgrt_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m __all__ = [\n",
            "\u001b[0;32mpmdarima/utils/_array.pyx\u001b[0m in \u001b[0;36minit pmdarima.utils._array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy pmdarima -y\n",
        "!pip install numpy==1.24.4\n",
        "!pip install --no-cache-dir pmdarima\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lz3-tfmWEZ3A",
        "outputId": "8e621daf-eeea-4b5d-e2ee-848231903811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: pmdarima 2.0.4\n",
            "Uninstalling pmdarima-2.0.4:\n",
            "  Successfully uninstalled pmdarima-2.0.4\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "scipy 1.16.0 requires numpy<2.6,>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.24.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0cdec65820d2464dac0a889d8e4b825d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.5.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.24.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.16.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (0.14.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.5.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (75.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
            "Collecting numpy>=1.21.2 (from pmdarima)\n",
            "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
            "Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m236.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pmdarima\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.1 pmdarima-2.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f411b36bd2b24744a88c3e05ee906bcf"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}